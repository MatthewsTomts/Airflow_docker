{"timestamp":"2025-05-31T21:53:29.301926","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-31T21:53:29.302743","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/projeto1/projeto1.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-31T21:53:29.663958Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T21:53:30.764444Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T21:53:30.764724Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T21:53:30.764856Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T21:53:31.401936Z","level":"error","event":"25/05/31 21:53:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.662730Z","level":"info","event":"44","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.720507Z","level":"info","event":"root","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.720720Z","level":"info","event":" |-- ID: integer (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.720804Z","level":"info","event":" |-- Age: integer (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.720886Z","level":"info","event":" |-- Email: string (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.720947Z","level":"info","event":" |-- JoinDate: date (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.721000Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-31T21:53:36.724460","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"UnsupportedOperation","exc_value":"not readable","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/projeto1/projeto1.py","lineno":45,"name":"start_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":2003,"name":"parquet"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1354,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1317,"name":"_build_args"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1304,"name":"_get_args"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_collections.py","lineno":510,"name":"convert"}]}]}
