{"timestamp":"2025-06-06T17:14:22.937499","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-06T17:14:22.938032","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/projeto_airflow/projeto_airflow.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-06T17:14:23.854586Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:25.330475Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:25.330868Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:25.331029Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:25.628933Z","level":"error","event":"25/06/06 17:14:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.107276Z","level":"info","event":"45","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431294Z","level":"info","event":"+---+-------+-------------------+-------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431476Z","level":"info","event":"| ID|    Age|              Email|           JoinDate|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431545Z","level":"info","event":"+---+-------+-------------------+-------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431595Z","level":"info","event":"|  0|     52|     grace@mail.com|2023-05-11 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431645Z","level":"info","event":"|  1|unknown|       bob@mail.com|2022-06-18 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431692Z","level":"info","event":"|  2|     41|  grace@example.com|2020-01-17 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431739Z","level":"info","event":"|  3|     38|   charlie@test.org|2022-07-24 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431786Z","level":"info","event":"|  4|     57|       eli@mail.com|2022-05-30 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431839Z","level":"info","event":"|  5|     20|     grace@mail.com|2022-12-12 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431921Z","level":"info","event":"|  8|     68|     grace@test.org|2023-06-14 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.431990Z","level":"info","event":"|  9|     55|       eli@test.org|2020-08-25 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432069Z","level":"info","event":"| 12|     27|    ian@example.com|2021-06-02 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432140Z","level":"info","event":"| 13|     43|    bob@example.com|2022-01-11 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432231Z","level":"info","event":"| 17|     26|     grace@mail.com|2023-01-18 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432314Z","level":"info","event":"| 18|     38|  grace@example.com|2022-12-02 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432387Z","level":"info","event":"| 19|     58|     frank@test.org|2022-05-18 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432467Z","level":"info","event":"| 21|     70|  frank@example.com|2021-01-24 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432541Z","level":"info","event":"| 22|     20|       ian@mail.com|2020-06-25 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432647Z","level":"info","event":"| 23|unknown|user[at]example.com|2023-04-11 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432733Z","level":"info","event":"| 24|     26|  frank@example.com|2023-10-26 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432808Z","level":"info","event":"| 25|     37|user[at]example.com|2023-12-27 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432871Z","level":"info","event":"| 26|     58|   user[at]mail.com|2021-05-25 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432919Z","level":"info","event":"| 27|     36|    hannah@test.org|2020-06-23 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.432963Z","level":"info","event":"| 29|   NULL|      dana@mail.com|2022-11-08 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433007Z","level":"info","event":"| 30|     40|       eli@mail.com|2023-06-05 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433051Z","level":"info","event":"| 31|   NULL|   charlie@mail.com|2020-09-03 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433094Z","level":"info","event":"| 33|     64|  grace@example.com|2021-07-11 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433138Z","level":"info","event":"| 34|   NULL|       ian@test.org|2022-09-01 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433183Z","level":"info","event":"| 35|     43|       bob@test.org|2021-03-24 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433251Z","level":"info","event":"| 36|     59| hannah@example.com|2020-10-27 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433297Z","level":"info","event":"| 38|     35|    ian@example.com|2022-02-12 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433342Z","level":"info","event":"| 39|     63|    bob@example.com|2020-12-07 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433388Z","level":"info","event":"| 40|     30|  frank@example.com|2022-10-04 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433431Z","level":"info","event":"| 42|     55|       eli@test.org|2021-09-14 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433476Z","level":"info","event":"| 44|     42|     alice@test.org|2020-04-24 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433521Z","level":"info","event":"| 46|     36|    hannah@test.org|2022-12-25 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433571Z","level":"info","event":"| 47|   NULL|       eli@test.org|2021-09-11 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433618Z","level":"info","event":"| 48|   NULL|     frank@test.org|2021-01-25 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433668Z","level":"info","event":"| 49|     46|     alice@test.org|2022-12-15 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433717Z","level":"info","event":"| 50|     27|       eli@mail.com|2020-09-12 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433758Z","level":"info","event":"| 51|     22|     grace@mail.com|2022-07-04 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433792Z","level":"info","event":"| 52|     29|  grace@example.com|2021-07-10 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433826Z","level":"info","event":"| 53|     30|    ian@example.com|2022-10-07 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433863Z","level":"info","event":"| 54|     52|     grace@test.org|2020-08-17 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433914Z","level":"info","event":"| 55|     32|  alice@example.com|2021-10-26 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433948Z","level":"info","event":"| 56|     57|       bob@mail.com|2021-08-28 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.433981Z","level":"info","event":"| 58|     47|charlie@example.com|2020-10-10 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434015Z","level":"info","event":"| 59|     64|    hannah@mail.com|2022-06-11 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434049Z","level":"info","event":"| 64|     51|       eli@mail.com|2023-03-15 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434082Z","level":"info","event":"| 65|   NULL|   user[at]mail.com|2023-01-28 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434116Z","level":"info","event":"| 66|     53|     alice@mail.com|2022-04-13 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434149Z","level":"info","event":"| 67|     56|      dana@mail.com|2023-01-10 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434183Z","level":"info","event":"| 68|     59|       eli@mail.com|2023-10-15 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434232Z","level":"info","event":"| 70|     44|  grace@example.com|2022-12-09 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434266Z","level":"info","event":"| 72|     43|       ian@test.org|2023-05-10 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434298Z","level":"info","event":"| 73|     47|     grace@test.org|2020-05-18 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434331Z","level":"info","event":"| 74|unknown|charlie@example.com|2022-03-28 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434364Z","level":"info","event":"| 75|     42|       ian@mail.com|2023-10-15 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434397Z","level":"info","event":"| 76|unknown|       ian@mail.com|2020-05-24 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434429Z","level":"info","event":"| 77|     56|    hannah@test.org|2022-06-15 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434462Z","level":"info","event":"| 78|     46|    hannah@test.org|2022-08-21 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434494Z","level":"info","event":"| 81|     54|   dana@example.com|2021-04-23 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434527Z","level":"info","event":"| 82|     54|   user[at]test.org|2020-06-28 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434564Z","level":"info","event":"| 83|     38|   user[at]mail.com|2022-07-01 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434598Z","level":"info","event":"| 84|     40|   user[at]mail.com|2020-08-18 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434630Z","level":"info","event":"| 89|     42|    hannah@mail.com|2021-07-27 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434663Z","level":"info","event":"| 90|     53|     grace@mail.com|2020-02-16 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434696Z","level":"info","event":"| 94|     23|      jill@mail.com|2023-03-09 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434728Z","level":"info","event":"| 95|     64|     grace@mail.com|2021-04-30 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434761Z","level":"info","event":"| 96|     30|       eli@test.org|2022-08-10 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434795Z","level":"info","event":"| 99|     38|    ian@example.com|2021-02-23 00:00:00|","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434841Z","level":"info","event":"+---+-------+-------------------+-------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.434876Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.476171Z","level":"info","event":"root","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.476370Z","level":"info","event":" |-- ID: integer (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.476432Z","level":"info","event":" |-- Age: integer (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.476484Z","level":"info","event":" |-- Email: string (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.476536Z","level":"info","event":" |-- JoinDate: date (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:32.476588Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.085638Z","level":"error","event":"25/06/06 17:14:33 ERROR Utils: Aborting task","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.085868Z","level":"error","event":"org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.085951Z","level":"error","event":"== DataFrame ==","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086009Z","level":"error","event":"\"cast\" was called from","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086069Z","level":"error","event":"/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086126Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086183Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086270Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086325Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086385Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086444Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086496Z","level":"error","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086543Z","level":"error","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086601Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086654Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086734Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086787Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086845Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086898Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086948Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.086997Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087044Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087092Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087136Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087181Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087253Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087298Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087341Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087396Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087440Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087483Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087525Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087567Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.087609Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.303039Z","level":"error","event":"\r[Stage 5:>                                                          (0 + 1) / 1]\r25/06/06 17:14:33 ERROR FileFormatWriter: Job: job_202506061714324325566026660857687_0005, Task: task_202506061714324325566026660857687_0005_m_000000, Task attempt attempt_202506061714324325566026660857687_0005_m_000000_5 aborted.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305375Z","level":"error","event":"25/06/06 17:14:33 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305535Z","level":"error","event":"org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305590Z","level":"error","event":"== DataFrame ==","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305628Z","level":"error","event":"\"cast\" was called from","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305701Z","level":"error","event":"/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305754Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305800Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305849Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305887Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305922Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305955Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.305989Z","level":"error","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306023Z","level":"error","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306058Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306092Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306126Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306159Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306194Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306260Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306296Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306330Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306364Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306413Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306448Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306485Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306532Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306572Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306618Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306664Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306706Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306752Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306799Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306845Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.306889Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320162Z","level":"error","event":"25/06/06 17:14:33 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (08137c4c4c00 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320360Z","level":"error","event":"== DataFrame ==","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320428Z","level":"error","event":"\"cast\" was called from","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320483Z","level":"error","event":"/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320535Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320590Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320643Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320694Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320744Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320794Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320861Z","level":"error","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320914Z","level":"error","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.320970Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321024Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321077Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321130Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321183Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321273Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321330Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321384Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321438Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321495Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321556Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321613Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321671Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321731Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321789Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321847Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321905Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.321963Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.322018Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.322088Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.322142Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.322197Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.322351Z","level":"error","event":"25/06/06 17:14:33 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328202Z","level":"error","event":"25/06/06 17:14:33 ERROR FileFormatWriter: Aborting job b378e9ca-7ea3-4d88-84c0-634a698f76db.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328357Z","level":"error","event":"org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328403Z","level":"error","event":"== DataFrame ==","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328440Z","level":"error","event":"\"cast\" was called from","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328476Z","level":"error","event":"/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328511Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328546Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328583Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328621Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328654Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328688Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328722Z","level":"error","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328756Z","level":"error","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328792Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328827Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328861Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328894Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328938Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.328972Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329006Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329040Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329074Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329107Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329140Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329174Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329218Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329258Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329292Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329326Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329360Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329394Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329428Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329462Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329495Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329528Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329561Z","level":"error","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329595Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329629Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329665Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329704Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329738Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329771Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329805Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329839Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329917Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329960Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.329996Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330031Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330065Z","level":"error","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330099Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330133Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330167Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330201Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330254Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330291Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330325Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330359Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330393Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330426Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330466Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330501Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330534Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330568Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330601Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330637Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330671Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330705Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330738Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330772Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330806Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330839Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330873Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330906Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330939Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.330973Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331007Z","level":"error","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331041Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331074Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331107Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331140Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331179Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331224Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331263Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331297Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331331Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331364Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331396Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331451Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331491Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331526Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331561Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331595Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331632Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331666Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331699Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331732Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331766Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.331799Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.456745Z","level":"error","event":"{\"ts\": \"2025-06-06 17:14:33.414\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'unknown' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"/opt/airflow/dags/projeto_airflow/projeto_airflow.py\", \"line\": \"48\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o87.parquet.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'unknown' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\n/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\\n\\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\\n\\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\\n\\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\\n\\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\\n\\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\\n\\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\\n\\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\\n\\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\\n\\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\\n\\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\t\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\t\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\t\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\t\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\t\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\t\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)\\n\\t\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\\n\\t\\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)\\n\\t\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\t\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\t\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\t\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\t\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\t\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\t\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\t\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\t\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\t\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\t\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\t\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\t\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\t\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\t\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\t\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\t\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\t\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\\n\\t\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\\n\\t\\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\\n\\t\\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\\n\\t\\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\\n\\t\\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\\n\\t\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\t\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\t\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\t\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\\n\\t\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-06T17:14:33.456654","level":"error","event":"[CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018","logger":"DataFrameQueryContextLogger","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o87.parquet.\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\n/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48\n\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n\t\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n\t\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n\t\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\n\t\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:107)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\t\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\t\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t\tat java.base/java.lang.Thread.run(Thread.java:840)\n\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":282,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":327,"name":"get_return_value"}]}]}
{"timestamp":"2025-06-06T17:14:33.498844","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"NumberFormatException","exc_value":"[CAST_INVALID_INPUT] The value 'unknown' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\n/opt/airflow/dags/projeto_airflow/projeto_airflow.py:48\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/projeto_airflow/projeto_airflow.py","lineno":53,"name":"transform_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":2003,"name":"parquet"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1362,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":288,"name":"deco"}]}]}
