{"timestamp":"2025-05-31T22:25:25.047677","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-31T22:25:25.048088","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/projeto1/projeto1.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-31T22:25:25.372548Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:26.538973Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:26.539157Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:26.539226Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:27.213184Z","level":"error","event":"25/05/31 22:25:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.270804Z","level":"error","event":"25/05/31 22:25:32 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.270986Z","level":"error","event":"org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'not_a_date' could not be parsed at index 0. Use `try_to_date` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271052Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271110Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271165Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271221Z","level":"error","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271277Z","level":"error","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271333Z","level":"error","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271386Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271440Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271493Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271547Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271600Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271680Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271755Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271808Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271865Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271917Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.271971Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.272022Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.272073Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.272123Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.272173Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.272222Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289362Z","level":"error","event":"25/05/31 22:25:32 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (a2616113db72 executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'not_a_date' could not be parsed at index 0. Use `try_to_date` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289515Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289560Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289598Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289658Z","level":"error","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289696Z","level":"error","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289732Z","level":"error","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289767Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289801Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289836Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289870Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289917Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289957Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.289991Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290025Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290059Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290093Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290127Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290160Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290194Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290228Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290261Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290295Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.290329Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.291632Z","level":"error","event":"25/05/31 22:25:32 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-31T22:25:32.314976","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"DateTimeException","exc_value":"[CANNOT_PARSE_TIMESTAMP] Text 'not_a_date' could not be parsed at index 0. Use `try_to_date` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/projeto1/projeto1.py","lineno":28,"name":"start_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py","lineno":285,"name":"show"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py","lineno":303,"name":"_show_string"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1362,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":288,"name":"deco"}]}]}
