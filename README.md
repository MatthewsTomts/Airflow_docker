
## üìö Qual a finalidade do projeto?

Esse projeto foi criado com o intuito de colocar em pr√°tica os conte√∫dos aprendidos sobre as ferramentas Airflow e Spark. Nele √© realizado o tratamento de dados presentes em um [arquivo csv](https://github.com/MatthewsTomts/Airflow_docker/blob/main/data/sample_with_errors_100.csv), em seguida exportar os dados para um arquivo no formato parquet e realizado o upload na ferramenta Minio e banco de dados PostgreSQL. 

---

<br>

## üíª Tecnologias ultilizadas

- **üêò PostgreSQL:** Ferramenta de banco de dados relacional;
- **‚≠ê Spark:** Ferramenta para tratamento e manipula√ß√£o de dados;
- **üüá  Airflow:** Orquestra√ß√£o de tarefas, altamente utilizado com tarefas de ETL;
- **üê≥ Docker:** Permite a cria√ß√£o e execu√ß√£o de ambiente isolados para aplica√ß√µes e processos;
- **üêç Python:** Linguagem de programa√ß√£o utilizada principalmente para processos de ETL;
- **ü¶¢ Minio:** Permite o deploy e utiliza√ß√£o de servidor de armazenamento de objetos, utilizando o padr√£o de API e uso do S3;
- **üñΩ  Parquet:** Formato de arquivo otimizado para consulta de dados, utilizando formato de armazenamento em colunas;
- **ùöá  CSV:** Formato de arquivo humano-leg√≠vel, utilizado para consultas simples e com armazenamento em linhas.


---

<br>

## Estrutura do Reposit√≥rio

- **/dags/aulas:**    

    Possui os arquivos de defini√ß√£o das DAGs criadas durante o per√≠odo de estudo da ferramente Airflow.
    Veja: [`/dags/aulas`](https://github.com/MatthewsTomts/Airflow_docker/tree/main/dags/aulas)

- **/dags/projeto_airflow:**

    Possui os arquivos de defini√ß√£o da DAG criada para o projeto pr√°tico que combina a utiliza√ß√£o das ferramentas Airflow e Spark. 
    veja: [`/dags/projeto_airflow`](https://github.com/matthewstomts/airflow_docker/tree/main/dags/projeto_airflow)

- **docker-compose.yaml**

    Arquivo de configura√ß√£o do ambiente de execu√ß√£o Airflow
    Veja: [`docker-compose.yaml`](https://github.com/MatthewsTomts/Airflow_docker/blob/main/docker-compose.yaml)

- **Dockerfile**

    Arquivo para customiza√ß√£o (extens√£o) da imagem base do Airflow, adicionando a biblioteca do PySpark 
    Veja: [`Dockefile`](https://github.com/MatthewsTomts/Airflow_docker/blob/main/Dockerfile)

- **/data**

    Pasta de armazenamento dos arquivos de dados utilizados durante as aulas e o projeto
    Veja: [`/data`](https://github.com/MatthewsTomts/Airflow_docker/tree/main/data)

---

<br>

## Como executar

### Opcional

1. Crie as vari√°veis de ambiente ```_AIRFLOW_WWW_USER_USERNAME``` e ```_AIRFLOW_WWW_USER_PASSWORD```. Caso as vari√°veis n√£o sejam criadas, o usu√°rio e senha do Airflow ter√£o valor "airflow" por padr√£o.  

### Passo a Passo

2. Certifique-se de ter o Docker instalado.
3. No terminal, dentro do diret√≥rio do reposit√≥rio, execute (substituindo o nome e a tag):
   
   ```sh
   docker build . --tag extending_airflow:latest 
   ```

4. Com a imagem personalizada criada, execute os containers utilizando:

    ```sh
    docker compose up -d 
    ```

5. Utilize o comando `docker ps` para identificar o id do container executando o postgres, acesse o container utilizando o comando `docker exec -it <id-do-container> bash`
6. Ao acessar o container do postgres, acesse o banco utilizando `psql -U airflow` e execute o comando `CREATE DATABASE test;`
7. Acesse o Airflow, na p√°gina `https://localhost:8080` utilizando o valor definido nas vari√°veis ou utilize o valor padr√£o
8. Na aba Admin, acesse a op√ß√£o conex√µes nela realize a cria√ß√£o de uma nova conex√£o do tipo "postgres", adicione as vari√°veis 
    ```
    host 'postgres'
    login <user-definido-em-docker-compose>
    senha <senha-definido-em-docker-compose>
    port 5432
    database test 
    ```
9. Ao navegar para a aba de DAG, √© poss√≠vel visualizar as DAGs criadas e execut√°-las

### Projeto

10. Para a execu√ß√£o do projeto, que possui o nome de DAG "Projeto_postgres", √© necess√°rio a execu√ß√£o da feramenta Minio (Modifique o usu√°rio e a senha):
    ```sh
    docker run \
    --network=airflow_docker_default \
    -p 9000:9000 \
    -p 9001:9001 \
    --user $(id -u):$(id -g) \
    --name minio1 \
    -e "MINIO_ROOT_USER=ROOTUSER" \
    -e "MINIO_ROOT_PASSWORD=CHANGEME123" \
    -v ${HOME}/minio/data:/data \
    quay.io/minio/minio server /data --console-address ":9001"
    ```

11. Acesse o link disponibilizado na execu√ß√£o acima com a identifica√ß√£o "API", realize o login e a cria√ß√£o de um novo bucket nomeando o "airflow" 
12. Retorne a p√°gina de conex√µes e crie uma nova conex√£o do tipo "aws", adicione o usu√°rio ao campo `AWS Access Key ID` e a senha ao `AWS Secret Access Key` e no campo `Extra Fields JSON` adicione o seguinte JSON:
    ```sh
    {
        "endpoint_url": "http://minio1:9000"
    }
    ```
13. Com esse passos √© poss√≠vel acionar a DAG do projeto.
